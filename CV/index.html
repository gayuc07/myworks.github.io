<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Gayathri Chandrasekaran</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/venobox/venobox.css" rel="stylesheet">
  <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: iPortfolio - v1.5.0
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <button type="button" class="mobile-nav-toggle d-xl-none"><i class="icofont-navigation-menu"></i></button>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/dp_round.jpg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Gayathri C</a></h1>
        <div class="social-links mt-3 text-center">        
          <a href="https://www.linkedin.com/in/gayathri-chandrasekaran-81169011b/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
          <a href="https://github.com/gayuc07" class="google-plus"><i class="bx bxl-github"></i></a>
          <a href="https://twitter.com/Gayathr35640281" class="twitter"><i class="bx bxl-twitter"></i></a> 
        </div>
      </div>

      <nav class="nav-menu">
        <ul>
          <li class="active"><a href="#hero"><i class="bx bx-home"></i> <span>Home</span></a></li>
          <li><a href="#resume"><i class="bx bx-file-blank"></i> <span>Resume</span></a></li>
          <li><a href="#portfolio"><i class="bx bx-book-content"></i> Projects</a></li>
        </ul>
      </nav><!-- .nav-menu -->
      <button type="button" class="mobile-nav-toggle d-xl-none"><i class="icofont-navigation-menu"></i></button>

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex flex-column justify-content-center align-items-center">
    <div class="hero-container" data-aos="fade-in">
      <h1>Gayathri Chandrasekaran</h1>
      <p><br>Data Science Graduate Student<br>George Washington University<br>Mail: gayuc07@gwu.edu<br>Phone: (202)-509-2284</p>
    </div>
  </section><!-- End Hero -->

  <main id="main">

    <!-- ======= Resume Section ======= -->
    <section id="resume" class="resume">
      <div class="container">

        <div class="section-title">
          <h2>Resume</h2>
        </div>

        <div class="row">
          <div class="col-lg-6" data-aos="fade-up">
            <h3 class="resume-title">Summary</h3>
            <div class="resume-item pb-0">
              <h4>Skills & Certifications</h4>
              <ul>
                <li><b>Languages: </b> R, Python, SQL, PL/SQL, JAVA</li>
                <li><b>Tools: </b> Tableau, Visual Studio Code</li>
                <li><b>Certifications: </b> ORACLE – SQL Fundamentals</li>
              </ul>
            </div>

            <h3 class="resume-title">Education</h3>
            <div class="resume-item">
              <h4>MS in Data Science</h4>
              <h5>Anticipated May 2021</h5>
              <p><em>George Washington University, DC</em></p>
              <p><b>Current CGPA (Spring 2020): 4.0/4.0</b> <br>Relevant Courses: Data Mining, Machine Learning, NLP, Cloud Computing, Time Series Analysis</p>
            </div>
            <div class="resume-item">
              <h4>B.E. in Electronics &amp; Communication</h4>
              <h5>Aug 2008 - June 2012</h5>
              <p><em>Velammal Engineering College (Anna University), India</em></p>
              <p><b>CGPA: 9.18/10</b><br>Honors: 1st rank in the college and 4th rank in the university.</p>
            </div>

            <h3 class="resume-title">Part-Time</h3>
            <div class="resume-item">
              <h4>Project Assistant II</h4>
              <h5>Jan 2020 - Present</h5>
              <p><em>George Washington University, DC</em></p>
              <ul>
                <li>Designed and built an interactive dashboard to demonstrate the significance of work produced by various GW
                  library departments using google charts.</li>
                <li>Analyzed and created customizable user reports for several classroom related services using tableau.
                </li>
              </ul>
            </div>
          </div>
          <div class="col-lg-6" data-aos="fade-up" data-aos-delay="100">
            <h3 class="resume-title">Professional Experience</h3>
            <div class="resume-item">
              <h4>IT Analyst</h4>
              <h5>Aug 2016 – July 2019</h5>
              <p><em>Tata Consultancy Services, Chennai, India </em></p>
              <ul>
                <li>Validated & tested predictive model to <b>forecast usage</b> of the customers and client’s revenue from past contracts.</li>
                <li>Designed and built an automated <b>regression testing tool</b> to compare testing images using OpenCV algorithm
                  which saved more than 70% of manual effort and improved accuracy. </li>
                <li>Trained team to design workflow, perform <b>ETL</b> operations and to identify KPI’s to promote business objectives.</li>
                <li>Performed data analysis, developed SQL queries, and worked in <b>Tableau</b> to generate customized client reports.</li>
              </ul>
              <p><b>Skills/Concepts – </b>Time Series Analysis, ARIMA, Open CV, Python, R, Data Warehousing, QlikView, Tableau, SQL</p>
            </div>
            <div class="resume-item">
              <h4>Assistant System Engineer</h4>
              <h5>Nov 2012 – Aug 2016</h5>
              <p><em>Tata Consultancy Services, Chennai, India </em></p>
              <ul>
                <li>Developed a tool using <b>UNIX Shell scripting</b> to monitor and improve the health of the WebLogic servers to
                  proactively track server status that helped to reduced <b>80%</b> of customer impacting incidents </li>
                <li>Provided superfast broadband access to customers by serving as core member at client office in United Kingdom.</li>
                <li>Widely appreciated for transforming new rack design workflow activities which brought new projects to TCS.</li>
              </ul>
              <p><b>Recognitions - </b>Star of the Month with several eCards of appreciation from the client and delivery manager</p>
              <p><b>Concepts – </b>WEBLOGIC, UNIX Shell Scripting, SQL</p>
            </div>
          </div>
        </div>

      </div>
    </section><!-- End Resume Section -->

    <!-- ======= Portfolio Section ======= -->
    <section id="portfolio" class="portfolio section-bg">
      <div class="container">

        <div class="section-title">
          <h2>Projects</h2>
        </div>

        <div class="row" data-aos="fade-up">
          <div class="col-lg-12 d-flex justify-content-center">
            <ul id="portfolio-flters">
              <li data-filter="*" class="filter-active">All</li>
              <li data-filter=".filter-app">Capital Bikeshare</li>
              <li data-filter=".filter-card">PBS Kids App</li>
              <li data-filter=".filter-web">NLP-SO Search Engine</li>
              <li data-filter=".filter-web1">Weather Forecasting</li>
              <li data-filter=".filter-web2">Traffic Congestion</li>
              <li data-filter=".filter-web3">Landmark Recognition</li>
            </ul>
          </div>
        </div>

        <div class="row portfolio-container" data-aos="fade-up" data-aos-delay="100">

          <div class="portfolio-item filter-app">
            <h3>Capital BikeShare</h3>
            <h6>Oct – Dec 2019</h6>
            <h4><b>Short Description</b></h4>
            <p>Developed a model to forecast the rental demand by predicting usage pattern using bikeshare and weather data</p>
            <p><b>Skills/Concepts –</b> Linear Regression, Random Forest, Decision Tree Classifiers</p>
            <p><b>git link: </b>https://github.com/gayuc07/Capital-Bikeshare</p>
            <img src="assets/img/cap_bikeshare/bike.JPG" class="img-fluid" alt="">
            <h4><b>Introduction</b></h4>
            <p>Capital Bikeshare (also called CapBi) is a bicycle sharing system that serves Washington DC, Arlington County, Alexandria, Falls Church, Montgomery County, Prince George’s County, and Fairfax County. The Capital Bikeshare system is owned by the local governments and is operated by Motivate International, Inc.(Motivate International, Inc). As of August 2019, Capital Bike has 500 stations and 4300 bicycles.</p>
            <p>The distribution of the docks is shown below:</p>
            <img src="assets/img/cap_bikeshare/map.JPG" class="img-fluid" alt="">
            <p>As we can see from the above image, the majority of the docks for the bicycle are in Washington DC.</p>
            <p>Bike tours in Washington DC are not only a popular family activity but renting a bike is a great way to get around without breaking the bank or sitting in traffic. There are dedicated bike lanes in Washington DC hence there is safety and convenience for the rider.</p>
            <p>Capital BikeShare is undoubtedly cheaper than its competitors and the docks are conveniently placed around monumental locations. Capital Bikeshare is often faster than other modes of transportation and its annual membership offers unlimited trips under 30 minutes which helps save money. CapBi can be used to commute to work or ride to meet friends and is a great alternative for exercise since it is human-powered instead of electric powered. CapBi services save fuel, prevents carbon emissions, it is not only healthy for the rider but also for the environment.</p>
            <p>As CapBi services are very popular and always in demand, we want to predict the number of bikes riders will use per hour and have contingencies to fulfill the demand. To estimate the number of bikes required we will consider various factors such as weather, temperature, working or non-working hour, the hour of the day, etc.</p>
            <h4><b>Exploratory Data Analysis</b></h4>
            <br>
            <h5><b>Bike Demand by Season and Temperature</b></h5>
            <p>From the plots below we can see that winter is the least favorite season for hiring bikes while spring, summer, and fall have pretty similar patterns.</p>
            <img src="assets/img/cap_bikeshare/bike_per_Season.JPG" class="img-fluid" alt="">
            <p>In this plot we also include the temperature, and observe that higher numbers of bikes are rented in each season when temperatures are between 80-90 degree Fahrenheit.</p>
            <img src="assets/img/cap_bikeshare/bike_weather_condition.JPG" class="img-fluid" alt="">
            <h5><b>Bike Demand by Year, Weekday & Hour of the Day</b></h5>
            <p>There is a steady increase in the number of bikes rented up to the year 2017 and then it decreased in 2018. Also, more bikes are hired during the weekday as compared to weekends.</p>
            <img src="assets/img/cap_bikeshare/bike_weekend.JPG" class="img-fluid" alt="">
            <p>The bikes hired peak during morning and evening 8 AM and 6 PM rush hours when people are heading or returning back from work.</p>
            <img src="assets/img/cap_bikeshare/bike_hour.JPG" class="img-fluid" alt="">
            <h5><b>Bike Demand by Holiday</b></h5>
            <p>We notice that riders rent bike more often on days when there is no holiday, but the number of bikes rented during holidays is still significant.</p>
            <img src="assets/img/cap_bikeshare/bike_holiday.JPG" class="img-fluid" alt="">
            <h5><b>Correlation Between Bikes Hired and Weather</b></h5>
            <p>There is a positive 44% correlation between temperature and bikes hired, additionally, Humidity has a negative correlation of 30%.</p>
            <img src="assets/img/cap_bikeshare/correlation.JPG" class="img-fluid" alt="">
            <h4><b>Model Assessment</b></h4>
            <p>Thus random forest performs the best with the R-square value being highest at 0.93.</p>
            <img src="assets/img/cap_bikeshare/Model_results.JPG" class="img-fluid" alt="">
            <img src="assets/img/cap_bikeshare/feature_imp.JPG" class="img-fluid" alt="">
            <h4><b>Conclusion</b></h4>
            <p>In terms of modelling & predictions, we can conclude that :</p>
            <ul>
              <li>Random Forest works best with the given dataset</li>
              <li>Maximum R2 value obtained is 0.93</li>
              <li>Variable Importance are as follows:
                <ul>
                  <li>Hour of Day – Best 6PM-7PM,8-9AM</li>
                  <li>Weekday – Weekend</li>
                  <li>Time of Day – Working Hour</li>
                  <li>Temp – Moderate Temperature – 70–90F</li>
                </ul>
              </li>
            </ul>
            <p>The insights which we got from our analysis is that on a normal day, users tend to ride a bike for commuting to offices, schools, etc. But on weekends & holidays, people prefer to use bikes for travel and leisure activity purposes. We also derive that bikes are preferred maximum in moderate temperatures and users tend to avoid bikes at high temperatures and low temperatures.</p>
            <p>Based on our analysis we recommend that during high demand in morning and evening office hours and weekend/holiday, Capital Bikeshare should increase availability during these hours. Thus catering to more users and in turn, securing more profits.</p>
            <h4><b>References</b></h4>
            <ul>
              <li>Motivate International, Inc. (n.d.). Press Kit. Retrieved November 26, 2019, from https://www.capitalbikeshare.com/press-kit.</li>
              <li>Capital Bikeshare Discount. (n.d.). Retrieved November 26, 2019, from https://benefits.gwu.edu/capital-bikeshare-discount.</li>
              <li>Therneau, T. M. (2019, April 11). An Introduction to Recursive Partitioning Using the RPART Routines. Retrieved November 26, 2019, from https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</li>
            </ul>
          </div>
          <div class="portfolio-item filter-card">
            <h3>PBS Kids Measure App Assessment</h3>
            <h6>Mar – May 2020</h6>
            <h4><b>Short Description</b></h4>
            <p>To predict the accuracy group of the user(i.e) to forecast how many attempts a child will take to pass a given assessment</p>
            <p><b>Skills/Concepts –</b> Linear Regression, Random Forest, Decision Tree Classifiers, Ensemble Methods</p>
            <p><b>git link: </b>https://github.com/gayuc07/PBS-KIDS-Measure-Up-App-Assesment</p>
            <img src="assets/img/PBS_Kids/app.JPG" class="img-fluid" alt="">
            <h4><b>Introduction</b></h4>
            <p>The "KIDS Measure Up" app is an educational application which helps kids explore and learn fundamental math concepts in a fun and interactive manner.</p>
            <p>The dataset contains game analytics for KIDS Measure Up! app. In this app, children navigate a map and complete various levels, which may be activities, video clips, games, or assessments.</p>
            <p>Each assessment is designed to test a child's comprehension of a certain set of measurement-related skills. There are five assessments: Bird Measurer, Cart Balancer, Cauldron Filler, Chest Sorter, and Mushroom Sorter.</p>
            <p>The purpose of our analysis is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment and hence predicting the accuracy group for user. The outcomes of this analysis are grouped into 4 groups (labeled accuracy_group in the data):</p>
            <ul>
              <li>3: the assessment was solved on the first attempt</li>
              <li>2: the assessment was solved on the second attempt</li>
              <li>1: the assessment was solved after 3 or more attempts</li>
              <li>0: the assessment was never solved</li>
            </ul>
            <h4><b>Exploratory Data Analysis</b></h4>
            <br>
            <h5><b>Assessment type</b></h5>
            <p>We notice that the Cart Balancer(Assessment) is the most popular assessment followed by Cauldron Filler, Mushroom Sorter, Chest Sorter and Bird Measurer.</p>
            <img src="assets/img/PBS_Kids/assess.JPG" class="img-fluid" alt="">
            <h5><b>GamePlay Type</b></h5>
            <p>We notice that the Clip media type is most famous, followed by Activity and Game.</p>
            <img src="assets/img/PBS_Kids/gameplay.JPG" class="img-fluid" alt="">
            <h5><b>Game Time</b></h5>
            <p>Accuracy group 0 is highest, it means that kids who take more attempts to solve the assessment, play the game for a longer time period.</p>
            <img src="assets/img/PBS_Kids/gametime.JPG" class="img-fluid" alt="">
            <h5><b>Assessment vs Accuracy Group</b></h5>
            <img src="assets/img/PBS_Kids/accuracy_grp.JPG" class="img-fluid" alt="">
            <h4><b>Modelling</b></h4>
            <p>As a Classification problem, we plan to experiment various classifier and identify best classifier that suit the given dataset.</p>
            <p>From exploratory data analysis and correlation matrix we could see most of the events id doesnt consitute to target values, so we aim to drop the feature from data. We trained whole data using random forest classifier and the event_id importance over the assesment target value is observed. Considering all these, we could able to select top 10 events that marginally influence the data.</p>
            <img src="assets/img/PBS_Kids/model_eval.JPG" class="img-fluid" alt="">
            <h5><b>Feature Importance</b></h5>
            <img src="assets/img/PBS_Kids/feature.JPG" class="img-fluid" alt="">
            <p>This is similar to EDA we analysed previously, each users events count, clips watched , game played before assessment are contributing factor for outcome of his assesment result. Also we could see,kid's experience over other assesment also helps them with current assesment.</p>
            <h4><b>Conclusion</b></h4>
            <br>
            <h5><b>EDA Results</b></h5>
            <ul>
              <li>Bird Measurer and Chest Sorter assesments are rare events, Kids usually prefer basic level assesment like cart balancer and cauldron filler than harder levels.</li>
              <li>TREETOPCITY is most played city by kids</li>
              <li>Kids are most interetsed to watch clips compared to games and activity</li>
              <li>From history of records provided, kids are able to perform well on their first attempt and falls into the accuracy group of 3. Kids are prone to complete assesments like cart balancer, cauldron Filler at their first attempt.</li>
              <li>Total time spent by group 3 is more compared to group 2 which infers kids take their time in their first assesment.</li>
              <li>Bird measurer and chest sorter have less accuracy level compared to other assesment.</li>
            </ul>
            <h5><b>Modelling</b></h5>
            <ul>
              <li>Ensemble methods and catboost model performs better with accuracy of ~56%</li>
              <li>Previous assesment details, event count, game time are influencing factors for predicting the target groups</li>
            </ul>
            <h5><b>Recommendations</b></h5>
            <p>From above analysis, we could suggest kids are finding difficulities in completing chest sorter, bird measurer, if they could add more clips and activity, it will be helpful to perform better. Also, as Kids prefered to watch more clips compared to games/activity, clip contents can be improved and they could also concentrate on developing eye catching game/activity or check for issues with present content for its less usage compared to clips, so that kids can take max out of these resources as well.</p>
            <h5>Future Goals</h5>
            <p>Present Model produces result at accuracy of ~56%.This is mainly due to high difference on number of samples of each accuracy group. Although oversampling is performed, model couldnt able to capture perfect distinction among the accuray groups. If we able to get more samples for these records then we could able infer better details. Also in this project we have experimented only with aggregated play history of each kids. However, there are possibility event sequence plays vital role, LSTM Model with embedding on events can be experimented to check sequence level dominance to target prediction.Further analysis on the dataset is required to get more prominent features that constitute to the target variables like checking interval after which each game or assesment is taken, etc.</p>

          </div>

          <div class="portfolio-item filter-web">
            <h3>Retrieval of Relevant Answers to Stack Overflow Queries by Ranking the Answers - Python</h3>
            <h6>Mar – May 2020</h6>
            <h4><b>Short Description</b></h4>
            <p>Improved Stack Overflow Q&A search using semantic similarity and ranking top answers for question searched.<br>Matched the question-answer pair using hierarchical cluster to find the best cluster using similarity score.</p>
            <p><b>Skills/Concepts –</b>  Word2Vec, Bert Embeddings, Cosine Similarity, Hierarchical Clustering</p>
            <p><b>git link: </b>https://github.com/gayuc07/NLP-StackOverflow-QnA-Search-Engine</p>
            <h4><b>Introduction</b></h4>
            <p>Stack Overflow is a question and answer site for professional and enthusiast programmers. It provides a platform for its users to post questions and answers. It also allows its users to upvote or downvote these questions and answers. Stack overflow has a wide range of questions and challenges faced by developers and their solutions in the computer engineering field. As per the Stack Overflow survey 2019, around 97% of developers visited the Stack Overflow website last year. And about 50 million people visit Stack Overflow to learn, share, and build their careers every month.</p>
            <p>Although this forum offers numerous advantages for developers, on the flip side there are few drawbacks associated with it. Sometimes Stack overflow’s search engine does not retrieve any results even if a similar question exists in the repository. And as stack overflow allows duplicate questions and each question has a different set of answers many users find it difficult to get relevant answers to their queries. As the main source of revenue for stack overflow is through advertising these are the advantages to drive more traffic. However, the problem arises when users use more sophisticated search engines like google and these search engines redirect the user to other sites. Hence to overcome these gaps this project aims at improving Stack Overflow search using semantic similarity between questions & answers and retrieving the most relevant answers using a ranking methodology.</p>
            <h4><b>Experimental Setup</b></h4>
            <img src="assets/img/NLP/Exper_setup.JPG" class="img-fluid" alt="">
            <p><b>Example:</b> Search Question: What is the use of lambda function?</p>
            <h5><b>Question Processing</b></h5>
            <p>In this phase, the probe question is compared with the stack overflow question corpus to find the most similar questions in the corpus. Initially, basic text preprocessing is performed on question title part and search question which includes lowercasing the tokens, converting accented characters, and removing stop words from tokens. However, stemming and lemmatization is not performed as the data is related to technical questions with different domain-specific meanings. Also, numeric data is preserved as removing numbers from questions changes the contextual meaning. On the preprocessed data various embedding techniques like word2vec, Bert, etc are performed to convert text to numbers. Embedded data is then compared using cosine similarity to get top similar questions for the search question. Each question is scored against distance value i.e. questions with less distance value are more similar to each other and vice versa. The questions with similar values are studied manually for various embedding techniques. It is observed that word2vec and Bert perform better than other embedding techniques for a given dataset. Hence, to improve the quality of the question similarity outcome results of both the embeddings are combined and questions with a similarity score greater than 0.90 are selected. Question processing helps gather answers spread across various duplicate questions for further analysis.</p>
            <img src="assets/img/NLP/Ques_process_manual.JPG" class="img-fluid" alt="">
            <p>For a given question questions.csv is explored to get similar questions and each question is ranked manually to compare and check the performance of model result as shown in Figure</p>
            <img src="assets/img/NLP/bert+word2.JPG" class="img-fluid" alt="">
            <p>As shown above fig the result obtained by the Bert+Word2vec model, 4 out of 5 questions match with our manual ranking.</p>
            <h5><b>Answer Processing</b></h5>
            <p>In this phase, associated answers for selected questions are filtered and grouped to form various answer clusters. Similar to the question processing, basic text preprocessing was performed on the answer dataset. Apart from that, HTML tags related to code, image, URL were removed to extract the text information from the answer corpus. This processed data is converted to numbers using embedding models like Bert, tf-idf, word2vec, and fed to the hierarchical clustering model. Like question processing, data clusters are evaluated manually for each embedding technique. It is observed that Bert clusters perform better compared to other cluster data. </p>
            <img src="assets/img/NLP/Answer_pro.JPG" class="img-fluid" alt="">
            <p>Figure shows hierarchical cluster results on answers set over various embedding models. After manual evaluation of each of the results, it is observed that Bert embedded results give good results with hierarchical clustering.</p>
            <h5><b>Question and Answer Mapping</b></h5>
            <p>Each answer cluster is compared to search questions using cosine similarity to check relevance between questions and answers. The similarity score is aggregated at a cluster level to rank the clusters. However, most of the answer part contains code chunks, leaving less information in the natural text to get actual relevance. To overcome this issue, the score features available in answer.csv. is used. The score value is the user annotated score for each answer to the question. Both similarity and user annotated scores are combined to get an overall score for each cluster. Clusters with maximum scores are ranked as top answer output.</p>
            <img src="assets/img/NLP/Q&A_rele.JPG" class="img-fluid" alt="">
            <h5><b>Top Answer</b></h5>
            <p>Top Answer obtained from our model is given below</p>
            <img src="assets/img/NLP/top_ans.JPG" class="img-fluid" alt="">
            <h4><b>Challenges</b></h4>
            <p>Stack Overflow Q&A system is domain-specific, every stage requires additional preprocessing to preserve domain details associated with it. To overcome the issue various data preprocessing steps were revisited to have maximum details for the next stage in the process.<br>The answer set is a mixture of text, codes, images, and links. The answer body is dominated with code chunks than natural text making it difficult to conclude only with the similarity score of the model. Different strategies were used to get the most relevant answers and to overcome the issue. </p>
            <h4><b>Results</b></h4>
            <p>The word relevance of word2vec combined with the Bert sequence model to capture the semantic importance of the word in numeric form works great for the dataset in retrieving relevant questions for search questions. Grouping the most similar answers from answer corpus helps with relevance matching and eliminates redundant answers from the result set. Overall, the Bert model produces good results for retrieval of matched answers for given questions.</p>
            <h4><b>References</b></h4>
            <ul>
              <li>Stack Overflow Developer Survey Results from 2019,Retrieved from:  https://insights.stackoverflow.com/survey/2019#overview</li>
              <li>Kaggle dataset - Python Questions from Stack Overflow - Retrieved from: https://www.kaggle.com/stackoverflow/pythonquestions</li>
              <li>Google Cloud Big Query StackOverflow Public dataset - Retrieved from: https://cloud.google.com/stack-overflow-q-a</li>
              <li>Training a Ranking Function for Open-Domain Question Answering Research Paper - Retrieved from: https://www.aclweb.org/anthology/N18-4017.pdf</li>
              <li>Comparison of different Word Embeddings on Text Similarity — A use case in NLP  - Retrieved from: https://medium.com/@Intellica.AI/</li>
              <li>Word embedding - Retrieved from: https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285</li>
            </ul>
          </div>
          <div class="portfolio-item filter-web1">
            <h3>Weather Forecasting - Hourly Temperature of Seattle City</h3>
            <h6>Oct – Dec 2020</h6>
            <h4><b>Short Description</b></h4>
            <p>Built an forecast model to predict the hourly temperature of seattle city</p>
            <p><b>Skills/Concepts –</b>  Stationarity Test, GPAC, Linear Regression, Auto regressive models(ARMA), Seasonal ARIMA</p>
            <p><b>git link: </b>https://github.com/gayuc07/Weather-Forecasting</p>
            <h4><b>Introduction</b></h4>
            <p>Weather forecast is the branch of science to predict the conditions of the atmosphere for the given location and time. This is more relatable as this helps to plan everyday travel and other related activities. Weather warnings are the most important forecasts as they protect life and property from adverse damage. In this project, I have used hourly temperature data of the Seattle city and built a prediction model to forecast the upcoming temperature. I have made use of various time series model techniques like average method, naïve method, drift method, simple exponential smoothing, holts’ linear method, holts winter method and ARMA methods to build the prediction model. Also, I have performed multivariate regression analysis on the dataset to check the linear dependency of the target variable with the regressor. A comparative study is performed to determine the best model by evaluating the results of these models like MSE values, variance & mean of the predicted error & forecast error, Q value and chi square test results. Using this best model,the h step forecast is performed on the test set.</p>
            <h5><b>Temperature Pattern Over time</b></h5>
            <img src="assets/img/Weather_Forecasting/temp.JPG" class="img-fluid" alt="">
            <p>From the above plot, we could observe strong trend, seasonality, and cyclic pattern present in the dataset.</p>
            <p>Let’s take a closer look at the seasonal part of the dataset, to understand more on the hourly data pattern. For this, I have sampled 7-day data from the dataset and plotted the data. </p>
            <img src="assets/img/Weather_Forecasting/7 day pattern.JPG" class="img-fluid" alt="">
            <p>This plot uses the data from July 28, 2016 00:00:00 to Aug 04,2016 23:00:00. We could see a repeating pattern every 24-time cycle. Also, seasonal spike is not the same, this information is helpful while selecting “add” or “mul” decomposition values in holts’ winter/linear method.
              Due to this cyclic nature and multiple seasonality, there is possibility that data may be highly nonlinear. To overcome this issue, I have resampled the data as per seasonal order and developed four models. Hence, data is sub-sampled as Spring data – March to May, Summer data – June to August, Fall data – September to November and Winter data – December to Feb. For this split, I have used 2016 data only. In this project, I have used Summer data to built the model and run the prediction over it. </p>
            <h5><b>Stationarity Test</b></h5>
            <img src="assets/img/Weather_Forecasting/stationarity_test1.JPG" class="img-fluid" alt="">
            <p>
              Although ADF test result suggests data is stationary with p value less than significance value of 0.05 and confidence interval of 95% as ASDF stats is less than 5% of the CI value, ACF plot lags values are decaying slowly with repeating pattern of 24 lags suggests that data is not stationary. This calls for the transformation like differencing. 
              Due to the seasonal nature of the data, I have performed seasonal differencing of period 24 over the data set. Let’s check the ACF plot for this differenced data</p>
            <img src="assets/img/Weather_Forecasting/stationarity_test2.JPG" class="img-fluid" alt="">
            <p>From the above plots, we could see that seasonal difference transformation has adjusted the repeating pattern to the maximum extent. However, we could still see the ACF is decaying slowly, hence I have used normal differential transformation on the data. </p>
            <img src="assets/img/Weather_Forecasting/stationarity_test3.JPG" class="img-fluid" alt="">
            <p>From the above plots and ADF tests we could see data became stationary with p value less then the significance value plus the ADF stats is far lower than 1% CI value suggesting more than 99% confidence interval. </p>
            <h4><b>Time Series Decomposition</b></h4>
            <p>I have used the additive STL decomposition to approximate trend, seasonality from the original dataset. </p>
            <img src="assets/img/Weather_Forecasting/time_Series_decompo.JPG" class="img-fluid" alt="">
            <p>From the plots, we could infer that STL decomposition works well for our data and able to capture most of the trends and seasonality present in our data. Variability present in the dataset is captured and removed from the data. Looks like both trend and seasonal components dominate our data, we will confirm the same as below</p>
            <img src="assets/img/Weather_Forecasting/stren.JPG" class="img-fluid" alt="">
            <p>We could see data has both trend and seasonality to the maximum, we could say there are higher chances that data might be nonlinear.</p>
            <h4><b>Conventional Basic Model</b></h4>
            <p>We have applied basic models like average, naïve, drift, SES, holts linear and holts winter method to our train set and made an h step prediction over the test data. All the basic stats values like MSE, Mean, Variance and Q value is calculated on the prediction error & forecast errors to do the comparative analysis over the model. </p>
            <img src="assets/img/Weather_Forecasting/conventional_model.JPG" class="img-fluid" alt="">
            <p>ACF plots also reveal that the holt’s winter one step prediction is almost equal to the impulse response (i.e.) white noise. Next closest model is holt’s linear model. Let’s give a closer look to the h step prediction of these models to understand the pattern. </p>
            <img src="assets/img/Weather_Forecasting/conve_acf.JPG" class="img-fluid" alt="">
            <img src="assets/img/Weather_Forecasting/conve_res.JPG" class="img-fluid" alt="">
            <p>Among base models, MSE of the holt’s winter method is lowest and the mean of the prediction error is almost 0 and variance is 0.37. Although it could not be able to capture exact variability, there exists some correlation between the predicted values and actual values. With Q value far less than other models, suggests holts winter method outperforms other basic conventional forecast models.</p>
            <h4><b>ARMA Models</b></h4>
            <br>
            <h5><b>Order Estimation</b></h5>
            <p>The potential order for the ARMA model can be calculated from the autocorrelation lag behavior present in the data. This checks possible correlation between the values to find best possible correlation value between the y(t) and y(t-h). let’s calculate ACF, PACF and GPAC to find the possible order for our data. </p>
            <img src="assets/img/Weather_Forecasting/gpac.JPG" class="img-fluid" alt="">
            <img src="assets/img/Weather_Forecasting/lagged_com.JPG" class="img-fluid" alt="">
            <p>From the GPAC, we could see the potential order to be as follows</p>
            <ol>
              <li>na = 2, nb = 0</li>
              <li>na = 3, nb = 0</li>
              <li>na = 4, nb = 1</li>
              <li>na = 6, nb = 5</li>
            </ol>
            <h4><b>ARMA(2,0) Model</b></h4>
            <h5><b>Paramter Estimation</b></h5>
            <img src="assets/img/Weather_Forecasting/arma_1_co.JPG" class="img-fluid" alt="">
            <p>Since Stats model package brings the co-efficient to the right side, the negative sign is neglected. We could see both the results match.</p>
            <p>ARMA (2,0) Model is given as</p>
            <img src="assets/img/Weather_Forecasting/arma_model.JPG" class="img-fluid" alt="">
            <h5><b>One step Prediction & Residuals</b></h5>
            <img src="assets/img/Weather_Forecasting/arma1_pl.JPG" class="img-fluid" alt="">
            <p>From the above one step prediction stats, we could see the mean and variance is not almost equal to the (0,1), also the Q value is huge. The plots suggest that the model couldn’t be predicted properly. Also, the residual plot is not close to white noise as we could see a sharp spike at the interval k = 24,48, etc. </p>
            <h5><b>Chi Square Test</b></h5>
            <p>From the residual plot, we could say that this model isn’t able to capture the entire information present in the data, as residuals do contain some information that are reflected by the large spike in the ACF. And the Q value is also high around 490. Let’s compare this value with q_critical and confirm the chi square test results on residual pattern</p>
            <img src="assets/img/Weather_Forecasting/chi_Sq1.JPG" class="img-fluid" alt="">
            <p>Chi square test results suggest that the residual errors are not white. Thus, chi square test failed for this model. This model is not the significant model.</p>
            <p>Let’s try the same approach for other estimated orders.</p>
            <h4><b>ARMA(3,0) Model</b></h4>
            <img src="assets/img/Weather_Forecasting/Arma2.JPG" class="img-fluid" alt="">
            <p>Since Stats model package brings the co-efficient to the right side, the negative sign is neglected. We could see both the results match.</p>
            <p>ARMA (3,0) Model is given as</p>
            <img src="assets/img/Weather_Forecasting/arma2_model.JPG" class="img-fluid" alt="">
            <h5><b>One step Prediction & Residuals</b></h5>
            <img src="assets/img/Weather_Forecasting/Arma2_plo.JPG" class="img-fluid" alt="">
            <p>From the above one step prediction stats, we could see the mean and variance is not almost equal to the (0,1), also the Q value is huge. The plots suggest that model couldn’t be
              predicted properly. Also, the residual plot is not close to white noise as we could see a sharp spike at the interval k = 24,48, etc. </p>
            <h5><b>Chi Square Test</b></h5>
            <p>From the residual plot, we could say that this model isn’t able to capture the entire information present in the data, as residuals do contain some information that are reflected by the large spike in the ACF. And the Q value is also high around 491. Let’s compare this value with q_critical and confirm the chi square test results on the residual pattern.</p>
            <img src="assets/img/Weather_Forecasting/chi_sq2.JPG" class="img-fluid" alt="">
            <p>Chi square test results suggest that the residual errors are not white. Thus, chi square test failed for this model. This model is not the significant model. </p>
            <p>I have also tried other na,nb values and chi square tests but failed for those models as well. One of the possible reasons for the chi square test failure may be non-linearity present in our data. Also, we have calculated the strength of the trend and seasonality in our dataset which is greater than 90%. Thus, remaining residuals dont constitute much to the data series. From the ACF of the residual values, we could see a sharp spike at lags k =24,48, this suggests there is more information left in the seasonal components of the dataset at the interval of 24. Thus, normal ARMA models do not produce good results with our dataset. </p>
            <p>As our data have high seasonality, I planned to experiment with the SARIMA model instead of ARIMA. Since ARIMA model expects the data to be non-seasonal, our earlier research suggests us that more information may be available at seasonal components, so I have skipped the ARIMA part and moved to the the SARIMA. </p>
            <h4><b>SARIMA - Seasonal ARIMA</b></h4>
            <p>The SARIMA model takes care of both seasonality and trend present in the data. The input to the model contains both seasonal components and non-seasonal components. And, the
              integration value in the components takes care of the trend present in the data. One major challenge with SARIMA is to find the order for the seasonal component. </p>
            <p>From the earlier analysis, we know that potential non-seasonal components might be ( 2,0) , (3,0),(4,1), (6,5). From the lagged ACF plot suggests that ACF is having a sharp spike and cutsoff after that, this gives a MA(1) model while PACF decays through the lags – this constitutes to the AR (0). Hence, potential order for the seasonal components would be (0,1). </p>
            <p>Lets built SARIMA model with these values and check for residuals and chi square test values for more information. </p>
            <p>Possible model – ARIMA (2,1,0) (0,1,1,24)</p>
            <img src="assets/img/Weather_Forecasting/sarima.JPG" class="img-fluid" alt="">
            <p>From the SARIMA outcome, we could see that Q value is dropped to 290 and all the p values of the coefficients are significant. But the prob of Q values is 0.00 which rejects the chi square test. The one potential reason that these linear methods are failing is due to the extreme seasonality and non-linearity present in the model. Although the Q value is dropped compared to the ARMA model, the MSE values are far greater than other models. Also Variance of 64.43 makes the estimators to be biased. </p>
            <img src="assets/img/Weather_Forecasting/arma_model_res.JPG" class="img-fluid" alt="">
            <p>Considering MSE and variance of the prediction error, we could say Holts winter is working good for the dataset. Also, residual function from the one step prediction is almost equal to white noise pattern. Hence, performance of the holt’s winter is best on all the forecast models experimented for the given dataset. </p>
            <h5><b>h step Prediction - Holts Winter Method</b></h5>
            <img src="assets/img/Weather_Forecasting/holts_winer_h_setp.JPG" class="img-fluid" alt="">
            <h4><b>Summary & Conclusions</b></h4>
            <p>Thus, we could say that Holt winter method fits our problem with least MSE values and variance on the prediction error, making it best among other models. The same procedure must be followed for other seasonal data splits. The major reason for drop in the performance by linear methods like ARMA, ARIMA, SARIMA is because of the non-linearity present in the model along with multiple seasonal patterns and cyclic behavior of the dataset. This makes it difficult for the linear methods. For future scope, we may want to explore other non-linear models like transfer function or neural nets to improve the performances. </p>
          </div>
          <div class="portfolio-item filter-web2">
            <h3>Geotab Intersection Congestion Prediction</h3>
            <h6>Mar – May 2020</h6>
            <h4><b>Short Description</b></h4>
            <p>Built an predictive model using amazon sagemaker to predict congesion by aggregate measure of stopping distance and waiting times.</p>
            <p><b>Skills/Concepts –</b>  Amazon Sagemaker, Kmeans Clustering, Random Forest</p>
            <p><b>git link: </b>https://github.com/gayuc07/Cloud_Computing-Traffic_Prediction</p>
            <h4><b>Introduction</b></h4>
            <img src="assets/img/traffic/intersec.JPG" class="img-fluid" alt="">
            <p>Intersection Congestion provides information gathered from commercial vehicle telematics devices.It includes aggregate stopped vehicle information and intersection wait times. 
              Possible use cases include(Geotab website):<br>A city can use the data to analyze congestion and light timing issues at scale across the region, and evaluate for impact or potential impact of infrastructure changes. 
              NGOs can use the data as part of research into emissions and pollution.<br>Fleet managers can use the information on traffic congestion at intersections as a factor to support optimal routing decisions.</p>
            <p>In this project, we use intersection details of Philadelphia city. Aim of this project is to make use of the amazon sagemaker service to predict congesion by aggregate measure of stopping distance and waiting times. </p>
            <p><b>Dataset: </b>https://www.kaggle.com/c/bigquery-geotab-intersection-congestion/data</p>
            <h4><b>Data Preprocessing</b></h4>
            <p>From Dataset, Features are mostly categorical values and they are nominal data. Inorder reduce the dimensionality of final dataset and preserve information present in each feature, we are clustering regions and forming new feature.</p>
            <h4><b>Cluster Details</b></h4>
            <img src="assets/img/traffic/Clusters.JPG" class="img-fluid" alt="">
            <h4><b>Exploratory Data Analysis</b></h4>
            <p>The cluster 3 & 4 had high congestion compared to the other cluster spot. The total time stopped at clusters 3 & 4 are having average greater than 20 to 30 seconds while cluster 1 has least time of about 10-12 seconds. </p>
            <img src="assets/img/traffic/EDA-Clusters_direction.JPG" class="img-fluid" alt="">
            <p>Higher rate of congestion happen at 8am in the morining and 4-5pm in the evening. Surprisingly, traffic congestion are more in the weekdays compared to the weekends.</p>
            <img src="assets/img/traffic/EDA-total_time_stopped.JPG" class="img-fluid" alt="">
            <h4><b>AWS Pipeline - Project Architecture & Data Flow</b></h4>
            <p>The aim of the project is to make of aws services like Amazon sagemaker, s3, e2 server, etc. The below data flow describes the end to end services used to build this project using AWS services.</p>
            <img src="assets/img/traffic/AWS_pipeline.JPG" class="img-fluid" alt="">
          </div>
          <div class="portfolio-item filter-web3">
            <h3>Popular Attraction/Landmark Recognition Using Google Landmark Dataset</h3>
            <h6>Oct – Dec 2019</h6>
            <h4><b>Short Description</b></h4>
            <p>Created a landmark recognition model to identify landmark present in image using google landmark dataset</p>
            <p><b>Skills/Concepts –</b>  HOG feature extractor, SVM classifier, Random Forest, Stacking ensemble Algorithm</p>
            <p><b>git link: </b>https://github.com/gayuc07/Landmark-Recognition</p>
            <h4><b>Introduction</b></h4>
            <p>With a rapid increase in the use of smartphones and other social apps, Image Recognition, Image Classification and Image Processing are the latest concepts that interest data engineers in computer vision tasks. A major challenge with image classification is the lack of a large, annotated dataset to train better and robust models.</p>
            <h4><b>Problem Statement</b></h4>
            <p>Recognizing and training the model to identify any landmark is a challenging task as the appearance of the landmark varies with geometry, illumination and a different aspect ratio of the image presented. To overcome this issue, a collection of images is used to capture typical appearance of the location. This project will focus to build a model that recognizes a given popular attraction or landmark using Google landmark dataset. This landmark recognition model will be handy to identify the name of a landmark in the image. This will also helpful for photo organization in smartphones and fields like aviation, maps, crime - solving, etc.</p>
            <h4><b>Dataset</b></h4>
            <p>In order to capture the typical appearance of an image via a collection of images, we need a large annotated landmark dataset. Google has released its latest landmark dataset named, GoogleLandmarks-v2 (September 2019) which makes it our ideal choice for landmark recognition and retrieval purposes. This dataset includes over 5 million images with more than 200,000 diverse landmark classes. Google has published this dataset in 3 sets – train, index and test. The train and test files are used for landmark recognition and index file is used for retrieval purposes. Train dataset consists of image details of various landmarks, while test dataset consists of images that include no landmark, one landmark or multiple landmark. The major challenge while using this dataset is that of a highly imbalanced training dataset. This is because since there are large number of categories, also many classes with single digit training data which makes it difficult to classify and train the model for such classes.</p>
            <ul>
              <li>Train dataset – 4132,914 location data with 203,094 unique classes</li>
              <li>Test dataset – 117,577 data points</li>
            </ul>
            <p>Since the dataset is highly imbalanced, performing data pre-processing needs to be considered before training the model. The dataset also needs to be cleaned to find any broken url (analyzing the image). The dataset is created by crowdsourcing the landmark available online. Each image might have different pixel size; hence these images need to be resized to one uniform pixel size for analysis and training. </p>
            <img src="assets/img/Landmark_recog/Freq_Plot.JPG" class="img-fluid" alt="">
            <h4><b>Algorithm Used</b></h4>
            <p>In this project, we have used HOG classifier for feature extraction and created comparative study how dataset reacts to various classifier like Logistic Regression, SVM, Naïve Bayes, KNN, Random Forest, Decision Tree and ensemble - Voting Classifier.</p>
            <h4><b>Experimental Setup</b></h4>
            <br>
            <h5><b>Data Load & Preprocessing</b></h5>
            <p>Train Dataset from Google landmark dataset is loaded, and top 10 sampled records are identified and stored. This data is used as source data for our project. Dataset is divided two parts – Train & Test sets. From image link given in URL, images are downloaded and saved in two folder – Train_image and Test_image. If image link in dataset is inaccessible or broken, id’s associated with data is added to errored is list. As downloaded image are of different dimension, to maintain uniformity, images are resized to aspect ratio – (256,256).</p>
            <ul>
              <li>Image_Download.py → This file describes image download and resize process. “download_prep” function is called from main function for every datapoint in train and test data. Once Image is processed, we have used HOG classifier for feature extraction from loaded images. </li>
              <li>Feature_Extraction.py – This file contains “hog” function – which calculates the gradients and orientation for each pixel values in image and histogram is derived for each cell. The feature details are then stacked as Numpy array and final Numpy array contains feature of the image is returned</li>
            </ul>
            <p>This process is repeated for all images in the dataset, resulting array is saved train feature and test feature list set respectively. Associated labels are saved to test labels and train labels. These are used as input and target variables. To save computational time, as data download and feature extraction for 30k dataset is huge, we have preloaded the data and csv files containing feature and label details are used for analysis purposes.</p>
            <h4><b>Modelling</b></h4>
            <p>As we have one input feature variable, we couldn’t able to hyper tune the parameters with respect to variables. We experimented with various model parameters that best fit for our dataset.</p>
            <ul>
              <li>Model_Function.py – This File contains model functions used for this project. It takes the train set feature and labels, fit the model and returns the predicted label set.</li>
            </ul>
            <h5><b>Model Comparison</b></h5>
            <img src="assets/img/Landmark_recog/acc.JPG" class="img-fluid" alt="">
            <p>From Accuracy Score and Kappa Score, we could say Random forest gives better accuracy rate and kappa value also falls under Fair agreement region, followed by Ensemble and Logistic Regression. SVM model has lowest accuracy and kappa score, hence, it doesn’t suit for given dataset.</p>
            <img src="assets/img/Landmark_recog/model_eval.JPG" class="img-fluid" alt="">
            <h5><b>Cross Validation Score</b></h5>
            <p>We reconfirm our result, we performed 10-fold cross-validation on trained set. Please find below result for the models.</p>
            <img src="assets/img/Landmark_recog/cv.JPG" class="img-fluid" alt="">
            <p>The cross-validation score is similar, we have Random forest, logistic regression with better score and SVM models has least value.</p>
            <h4><b>Conclusion</b></h4>
            <p>Landmark recognition model is built to classify top 10 sampled landmark id of google dataset. This project explored the possibility of building model with various machine learning algorithm. From comparative study, we could see Random Forest algorithm works best for given dataset followed by logistic Regression. In terms of ensemble model, Random forest with nonlinear SVM gives better classification Model. SVM model doesn’t suit for our dataset. As dataset is highly imbalance, its hard to find optimum boundary using SVM. Thus, random forest well suited for our landmark recognition data. However, Accuracy achieved is 68%, which is not great. As dataset is huge and imbalance, if we increase class scalability, these algorithms may not work best for recognizing landmark. In such cases we can use neural network may works better. Also, many classes have least datapoints, if we get more annotated images, prediction percentage may increase further.</p>
            <h4><b>References</b></h4>
            <ul>
              <li>Announcing Google-Landmarks-v2: An Improved Dataset for Landmark Recognition & Retrieval (2019, September), Retrieved from: https://ai.googleblog.com/2019/05/announcing-google-landmarks-v2-improved.html</li>
              <li>The Common Visual Data Foundation(2019, September), Google Landmarks Dataset v2, Retrieved from: https://www.kaggle.com/c/landmark-recognition-2019</li>
              <li>Y. Li, D. J. Crandal and D. P. Huttenlocher, Landmark Classification in Large-scale Image Collections, Retrieved from: https://www.cs.cornell.edu/~yuli/papers/landmark.pdf</li>
              <li>A. Crudge, W. Thomas and K. Zhu, Landmark Recognition Using Machine Learning, Retrieved from: http://cs229.stanford .edu/proj2014/Andrew%20Crudge, %20Will%20Thomas,%20Kaiyuan%20Zhu,%20Landmark%20Recognition%20Using%20Machine%20Learning.pdf</li>
              <li>Y. Takeuchi, P. Gros, M. Hebert and K. Ikeuchi, Visual Learning for Landmark Recognition, Retrieved from: https://www.cs.cmu.edu/~takeuchi/iuw97/iuw97.html https://www.Analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/</li>
              <li>HOG Classifier Feature Engineering for Images: A Valuable Introduction to the HOG Feature Descriptor - Retrieved from:https://www.analyticsvidhya.com/blog/2019/09/feature-engineering-images-introduction-hog-feature-descriptor/</li>
            </ul>
          </div>
        </div>
      </div>
    </section><!-- End Portfolio Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>iPortfolio</span></strong>
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- End  Footer -->

  <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/waypoints/jquery.waypoints.min.js"></script>
  <script src="assets/vendor/counterup/counterup.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/venobox/venobox.min.js"></script>
  <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>